{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env \n",
    "from gym import spaces \n",
    "import random \n",
    "import numpy as np \n",
    "from IPython.display import clear_output\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game board values\n",
    "NOTHING = 0\n",
    "PLAYER = 1\n",
    "WIN = 2\n",
    "LOSE_T = 50\n",
    "# action values\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEnv(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        # custom class variable used to display the reward earned\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        # set the initial state to a flattened 6x6 grid with a randomly placed entry, win, and player\n",
    "        \n",
    "        self.state = [NOTHING] * 36 \n",
    "        self.player_position = random.randrange(0, 35)\n",
    "        self.win_position = 35\n",
    "        self.lose_time = 0\n",
    "        \n",
    "\n",
    "        self.state[self.player_position] = PLAYER\n",
    "        self.state[self.win_position] = WIN\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        self.state = np.array(self.state)\n",
    "    \n",
    "        \"\"\"\n",
    "            observation space (valid ranges for observations in the state)\n",
    "            creates a space where each of the 36 elements can range from 0 to 3 (0,1,2,3)\n",
    "\n",
    "            low: A float or array specifying the lower bounds of the box.\n",
    "            high: A float or array specifying the upper bounds of the box.\n",
    "            shape: (Optional) A tuple specifying the shape of the box. If not provided, it’s inferred from low or high.\n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "        self.observation_space = spaces.Box(low = 0, high = 2, shape= [36,]) \n",
    "        \"\"\"\n",
    "        Valid actions:\n",
    "           0 = up\n",
    "           1 = down\n",
    "           2 = left\n",
    "           3 = right\n",
    "\n",
    "         spaces.Discrete(4) creates a discrete space with 4 possible actions, which are represented by integers 0, 1, 2, and 3.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    def step(self, action):\n",
    "        # placeholder for debugging information\n",
    "        info = {}\n",
    "        # set default values for done, reward, and the player position before taking the action\n",
    "\n",
    "        done = False\n",
    "        reward = 0.01\n",
    "        previous_position = self.player_position\n",
    "        \n",
    "\n",
    "        \"\"\" \n",
    "        take the action by moving the player\n",
    "        The grid was transform as a vector of 36 components try to represent the grid and move to understand \n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if action == UP:\n",
    "            if (self.player_position - 6) >= 0:\n",
    "                    self.player_position -= 6\n",
    "        elif action == DOWN:\n",
    "            if (self.player_position + 6) < 36:\n",
    "                    self.player_position += 6\n",
    "        elif action == LEFT:\n",
    "            if (self.player_position % 6) != 0:\n",
    "                    self.player_position -= 1\n",
    "        elif action == RIGHT:\n",
    "            if (self.player_position % 6) != 5:\n",
    "                    self.player_position += 1\n",
    "        else:\n",
    "            # check for invalid actions\n",
    "            raise Exception(\"invalid action\")\n",
    "        #\n",
    "        # check for win/lose conditions and set reward\n",
    "        #\n",
    "        self.lose_time += 1\n",
    "        \n",
    "        if self.player_position==previous_position:\n",
    "             reward = -0.5\n",
    "        \n",
    "        if self.state[self.player_position] == WIN:\n",
    "            reward = 1.0\n",
    "            self.cumulative_reward += reward\n",
    "            done = True    \n",
    "            \n",
    "         \n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print('YOU WIN!!!!')\n",
    "\n",
    "        elif self.lose_time == LOSE_T:\n",
    "             reward = -2.0\n",
    "             self.cumulative_reward += reward \n",
    "             done = True\n",
    "             print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "             print('YOU LOSE')\n",
    "        #\n",
    "        # Update the environment state\n",
    "        #\n",
    "        if not done:\n",
    "            # update the player position\n",
    "            self.state[previous_position] = NOTHING\n",
    "            self.state[self.player_position] = PLAYER\n",
    "        self.cumulative_reward += reward\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_reward = 0\n",
    "        #\n",
    "        # set the initial state to a flattened 6x6 grid with a randomly placed entry, win, and player\n",
    "        #\n",
    "        self.state = [NOTHING] * 36\n",
    "\n",
    "        self.player_position = random.randrange(0, 35)\n",
    "        self.win_position = 35\n",
    "       \n",
    "        self.lose_time = 0\n",
    "        \n",
    "      \n",
    "\n",
    "   \n",
    "            \n",
    "        self.state[self.player_position] = PLAYER\n",
    "        self.state[self.win_position] = WIN\n",
    "        \n",
    "\n",
    "        # convert the python array into a numpy array (needed since Gym expects the state to be this way)\n",
    "        self.state = np.array(self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def render(self):\n",
    "        # visualization can be added here\n",
    "        pretty_print(self.state, self.cumulative_reward)\n",
    "\n",
    "def pretty_print(state_array, cumulative_reward):\n",
    "    #clear_screen()\n",
    "    print(f'Cumulative Reward: {cumulative_reward}')\n",
    "    print()\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            print('{:4}'.format(state_array[i*6 + j]), end = \"\")\n",
    "        print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states,nodes,actions) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # DQN layers \n",
    "        self.input = nn.Linear(in_states,nodes) # Couche d'entré \n",
    "        self.out = nn.Linear(nodes,actions) #Couche de sortie\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "#Experience Replay\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self,maxlen) -> None:\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    def append(self,value):\n",
    "        self.memory.append(value)\n",
    "    def sample(self, s_size):\n",
    "        return random.sample(self.memory,s_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_input(state,nb_states)->torch.Tensor:\n",
    "    player = np.where(state == 1)[0]\n",
    "    win = np.where(state == 2)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    input_tensor = torch.zeros(nb_states)\n",
    "    input_tensor[player] = 1\n",
    "    input_tensor[win] = 2 \n",
    "        \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_P():\n",
    "    \n",
    "    learning_rate = 0.001         \n",
    "    discount_factor = 0.9               \n",
    "    network_sync_rate = 10          # Nombre d'etape que l'agent prend avant la synchronisation de la policy et la target network\n",
    "    replay_memory_size = 1000       # Taille de la replay memory\n",
    "    mini_batch_size = 32         \n",
    "\n",
    "    # Neural Network\n",
    "    loss = nn.MSELoss()            \n",
    "    optimizer = None                \n",
    "\n",
    "    #ACTIONS = ['U','D','L','R']  \n",
    "    \n",
    "\n",
    "    def train(self, episodes):\n",
    "      env = BasicEnv()\n",
    "\n",
    "      nb_states = env.observation_space.shape[0]\n",
    "      nb_actions = env.action_space.n\n",
    "      \n",
    "  \n",
    "      epsilon = 1 # 1 = 100% d'actions aléatoire\n",
    "      memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "      # policy and target network.\n",
    "      policy_dqn = DQN(in_states=nb_states, nodes=nb_states, actions=nb_actions)\n",
    "      target_dqn = DQN(in_states=nb_states, nodes=nb_states, actions=nb_actions)\n",
    "\n",
    "      target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate)\n",
    "\n",
    "      rewards_par_episode = np.zeros(episodes)\n",
    "\n",
    "      epsilon_history = []\n",
    "\n",
    "      step_count=0\n",
    "\n",
    "      for i in range(episodes):\n",
    "        print(\"EPISODES = \",i)\n",
    "        state = env.reset()\n",
    "        done = False \n",
    "        \n",
    "        \n",
    "        while (not done ):\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            else :\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(state_to_input(state,nb_states)).argmax().item()\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "            memory.append((state, action, new_state, reward, done)) \n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            step_count += 1\n",
    "            \n",
    "\n",
    "        if reward == 1 :\n",
    "            rewards_par_episode[i] = 1\n",
    "\n",
    "        if len(memory)>self.mini_batch_size and np.sum(rewards_par_episode)>0:\n",
    "            mini_batch = memory.sample(self.mini_batch_size)\n",
    "            self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "            # Decay epsilon\n",
    "            epsilon = max(epsilon - 1/episodes, 0)\n",
    "            epsilon_history.append(epsilon)\n",
    "\n",
    "            \n",
    "            # Copie de policy network vers target network apres un certain nb d'etape\n",
    "            if step_count > self.network_sync_rate:\n",
    "                target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                step_count=0\n",
    "                \n",
    "     \n",
    "\n",
    "      env.close()\n",
    "\n",
    "\n",
    "      torch.save(policy_dqn.state_dict(), \"game_dql.pt\")\n",
    "    \n",
    "    # Optimize policy \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        # Get nb of input nodes\n",
    "        nb_states = policy_dqn.input.in_features\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, done in mini_batch:\n",
    "\n",
    "\n",
    "            if done: \n",
    "              \n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value decsente de gradient (Q learning Formula)\n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor * target_dqn(state_to_input(new_state,nb_states)).max()\n",
    "                    )\n",
    "\n",
    "            # recuppérer les valeurs de Q values\n",
    "            current_q = policy_dqn(state_to_input(state,nb_states))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(state_to_input(state,nb_states)) \n",
    "            # Lier l'action au reward\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "                \n",
    "        # loss\n",
    "        loss = self.loss(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Descente de gradient\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def test(self, episodes):\n",
    "        \n",
    "        env = BasicEnv()\n",
    "        nb_states = env.observation_space.shape[0]\n",
    "        nb_actions = env.action_space.n\n",
    "        \n",
    "\n",
    "        # charger la policy sauvegardé\n",
    "        policy_dqn = DQN(in_states=nb_states, nodes=nb_states, actions=nb_actions) \n",
    "        \n",
    "        policy_dqn.load_state_dict(torch.load(\"game_dql.pt\"))\n",
    "        \n",
    "        policy_dqn.eval()    # eval model\n",
    "        \n",
    "\n",
    "\n",
    "        for i in range(episodes):\n",
    "            print(\"EPISODES = \",i)\n",
    "            state = env.reset()\n",
    "            done = False \n",
    "                              \n",
    "\n",
    "            \n",
    "            while(not done ):  \n",
    "                \n",
    "                  \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(state_to_input(state,nb_states)).argmax().item()\n",
    "                    \n",
    "\n",
    "                \n",
    "                state,reward,done,info = env.step(action)\n",
    "                \n",
    "               \n",
    "                env.render()\n",
    "\n",
    "        env.close()    \n",
    "\n",
    "\n",
    "          \n",
    "          \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = Game_P()\n",
    "Game.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
